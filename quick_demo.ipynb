{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yzhou359/MakeItTalk/blob/main/quick_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXaL7nU6TEsV"
      },
      "source": [
        "# MakeItTalk Quick Demo (natural human face animation)\n",
        "\n",
        "- included project setup + pretrained model download\n",
        "- provides step-by-step details\n",
        "- todo: tdlr version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "olj6VcfiTrd_"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"thirdparty/AdaptiveWingLoss\")\n",
        "import os, glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import argparse\n",
        "from src.approaches.train_image_translation import Image_translation_block\n",
        "import torch\n",
        "import pickle\n",
        "import face_alignment\n",
        "from src.autovc.AutoVC_mel_Convertor_retrain_version import AutoVC_mel_Convertor\n",
        "import shutil\n",
        "import time\n",
        "import util.utils as util\n",
        "from scipy.signal import savgol_filter\n",
        "from src.approaches.train_audio2landmark import Audio2landmark_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8aaCE6vgmXy"
      },
      "source": [
        "## Step 1: Basic setup for the animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "58s-c9H8dWPW"
      },
      "outputs": [],
      "source": [
        "default_head_name = 'paint_boy'           # the image name (with no .jpg) to animate\n",
        "ADD_NAIVE_EYE = True                 # whether add naive eye blink\n",
        "CLOSE_INPUT_FACE_MOUTH = False       # if your image has an opened mouth, put this as True, else False\n",
        "AMP_LIP_SHAPE_X = 2.                 # amplify the lip motion in horizontal direction\n",
        "AMP_LIP_SHAPE_Y = 2.                 # amplify the lip motion in vertical direction\n",
        "AMP_HEAD_POSE_MOTION = 0.7           # amplify the head pose motion (usually smaller than 1.0, put it to 0. for a static head pose)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRFBOqXMguSH"
      },
      "source": [
        "Default hyper-parameters for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZkZRYLSCf8TK"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--jpg', type=str, default='{}.jpg'.format(default_head_name))\n",
        "parser.add_argument('--close_input_face_mouth', default=CLOSE_INPUT_FACE_MOUTH, action='store_true')\n",
        "\n",
        "parser.add_argument('--load_AUTOVC_name', type=str, default='examples/ckpt/ckpt_autovc.pth')\n",
        "parser.add_argument('--load_a2l_G_name', type=str, default='examples/ckpt/ckpt_speaker_branch.pth')\n",
        "parser.add_argument('--load_a2l_C_name', type=str, default='examples/ckpt/ckpt_content_branch.pth') #ckpt_audio2landmark_c.pth')\n",
        "parser.add_argument('--load_G_name', type=str, default='examples/ckpt/ckpt_116_i2i_comb.pth') #ckpt_image2image.pth') #ckpt_i2i_finetune_150.pth') #c\n",
        "\n",
        "parser.add_argument('--amp_lip_x', type=float, default=AMP_LIP_SHAPE_X)\n",
        "parser.add_argument('--amp_lip_y', type=float, default=AMP_LIP_SHAPE_Y)\n",
        "parser.add_argument('--amp_pos', type=float, default=AMP_HEAD_POSE_MOTION)\n",
        "parser.add_argument('--reuse_train_emb_list', type=str, nargs='+', default=[]) #  ['iWeklsXc0H8']) #['45hn7-LXDX8']) #['E_kmpT-EfOg']) #'iWeklsXc0H8', '29k8RtSUjE0', '45hn7-LXDX8',\n",
        "parser.add_argument('--add_audio_in', default=False, action='store_true')\n",
        "parser.add_argument('--comb_fan_awing', default=False, action='store_true')\n",
        "parser.add_argument('--output_folder', type=str, default='examples')\n",
        "\n",
        "parser.add_argument('--test_end2end', default=True, action='store_true')\n",
        "parser.add_argument('--dump_dir', type=str, default='', help='')\n",
        "parser.add_argument('--pos_dim', default=7, type=int)\n",
        "parser.add_argument('--use_prior_net', default=True, action='store_true')\n",
        "parser.add_argument('--transformer_d_model', default=32, type=int)\n",
        "parser.add_argument('--transformer_N', default=2, type=int)\n",
        "parser.add_argument('--transformer_heads', default=2, type=int)\n",
        "parser.add_argument('--spk_emb_enc_size', default=16, type=int)\n",
        "parser.add_argument('--init_content_encoder', type=str, default='')\n",
        "parser.add_argument('--lr', type=float, default=1e-3, help='learning rate')\n",
        "parser.add_argument('--reg_lr', type=float, default=1e-6, help='weight decay')\n",
        "parser.add_argument('--write', default=False, action='store_true')\n",
        "parser.add_argument('--segment_batch_size', type=int, default=1, help='batch size')\n",
        "parser.add_argument('--emb_coef', default=3.0, type=float)\n",
        "parser.add_argument('--lambda_laplacian_smooth_loss', default=1.0, type=float)\n",
        "parser.add_argument('--use_11spk_only', default=False, action='store_true')\n",
        "parser.add_argument('-f')\n",
        "\n",
        "opt_parser, unknown = parser.parse_known_args()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qchIUwTTg3AB"
      },
      "source": [
        "## Step 2: load the image and detect its landmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SmYcSmrugxQK"
      },
      "outputs": [],
      "source": [
        "img =cv2.imread('AI-square.jpg')\n",
        "img = cv2.resize(img, (256, 256))\n",
        "predictor = face_alignment.FaceAlignment(face_alignment.LandmarksType.THREE_D, device='cpu', flip_input=True)\n",
        "shapes = predictor.get_landmarks(img)\n",
        "if (not shapes or len(shapes) != 1):\n",
        "    print('Cannot detect face landmarks. Exit.')\n",
        "    exit(-1)\n",
        "shape_3d = shapes[0]\n",
        "\n",
        "if(opt_parser.close_input_face_mouth):\n",
        "    util.close_input_face_mouth(shape_3d)\n",
        "\n",
        "shape_3d, scale, shift = util.norm_input_face(shape_3d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-97.5 [-131.  -110.5]\n"
          ]
        }
      ],
      "source": [
        "print(256 / (scale * 256), shift)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "97.5\n"
          ]
        }
      ],
      "source": [
        "print(256 / (0.010256410256410256 * 256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1357\n"
          ]
        }
      ],
      "source": [
        "#read json file\n",
        "import json\n",
        "with open('landmarks_annie.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "data_fls = []\n",
        "for k, v in data.items():\n",
        "    data_fls.append(v)\n",
        "\n",
        "print(len(data_fls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 6.48250000e+01  1.10500000e+02 -1.82168450e+00  5.86153750e+01\n",
            "   1.32527200e+02 -1.87465430e+00  6.20971000e+01  1.52511775e+02\n",
            "  -1.80245510e+00  6.57874750e+01  1.68132250e+02 -1.56746070e+00\n",
            "   7.12442000e+01  1.87141825e+02 -1.21938810e+00  8.30816750e+01\n",
            "   2.03738275e+02 -5.87100100e-01  9.42464000e+01  2.14600750e+02\n",
            "   4.48520560e-01  1.06849250e+02  2.23722850e+02  1.35351060e+00\n",
            "   1.29641825e+02  2.29153600e+02  1.37948620e+00  1.53342750e+02\n",
            "   2.24697850e+02  1.08266510e+00  1.68938225e+02  2.14600750e+02\n",
            "  -4.07776440e-01  1.82725700e+02  2.02763275e+02 -2.15717200e+00\n",
            "   1.93588175e+02  1.87141825e+02 -3.40222300e+00  1.98347150e+02\n",
            "   1.68132250e+02 -4.12142570e+00  2.01827900e+02  1.51536775e+02\n",
            "  -4.47277670e+00  2.05309625e+02  1.32527200e+02 -4.45581600e+00\n",
            "   2.05100000e+02  1.10500000e+02 -4.50210140e+00  6.85278500e+01\n",
            "   9.71308000e+01 -7.33965400e-01  7.86249500e+01  8.68000500e+01\n",
            "  -9.61628000e-02  9.46646750e+01  8.77750500e+01  2.17981180e-01\n",
            "   1.02602150e+02  8.95154250e+01  4.05233630e-01  1.15414625e+02\n",
            "   9.51808000e+01  6.51221930e-01  1.46726400e+02  9.42058000e+01\n",
            "   8.87686900e-02  1.59816125e+02  8.97500500e+01 -4.30160400e-01\n",
            "   1.71653600e+02  8.57324250e+01 -1.03508630e+00  1.85232425e+02\n",
            "   8.58250500e+01 -1.86040250e+00  1.95630800e+02  9.42058000e+01\n",
            "  -2.83821360e+00  1.29851450e+02  1.15931725e+02  1.63531240e+00\n",
            "   1.29851450e+02  1.32527200e+02  2.22483850e+00  1.35851450e+02\n",
            "   1.43389675e+02  2.31485680e+00  1.35851450e+02  1.51327150e+02\n",
            "   2.39113790e+00  1.13811725e+02  1.55876500e+02  9.21244650e-01\n",
            "   1.22469725e+02  1.61726500e+02  1.34355300e+00  1.30616825e+02\n",
            "   1.60541875e+02  1.61742000e+00  1.40923550e+02  1.61726500e+02\n",
            "   1.48100050e+00  1.51045650e+02  1.56851500e+02  1.08916120e+00\n",
            "   7.82066750e+01  1.12240375e+02 -1.16739670e+00  8.86050500e+01\n",
            "   1.06600000e+02 -7.64700800e-01  1.08602150e+02  1.06600000e+02\n",
            "  -8.70545050e-01  1.12489625e+02  1.15931725e+02 -8.59677700e-01\n",
            "   1.01627150e+02  1.19622100e+02 -9.67144850e-01  8.66550500e+01\n",
            "   1.19622100e+02 -1.13418500e+00  1.50417750e+02  1.17115375e+02\n",
            "  -1.62563820e+00  1.60791125e+02  1.06600000e+02 -1.83897560e+00\n",
            "   1.74578600e+02  1.06600000e+02 -2.01862980e+00  1.92625700e+02\n",
            "   1.12240375e+02 -2.56858800e+00  1.75553600e+02  1.19622100e+02\n",
            "  -2.26382910e+00  1.59816125e+02  1.20806725e+02 -1.87555850e+00\n",
            "   1.04343500e+02  1.86166825e+02  1.18435140e+00  1.14230975e+02\n",
            "   1.80439675e+02  1.09287980e+00  1.24419725e+02  1.76254975e+02\n",
            "   1.25629240e+00  1.30641200e+02  1.77995350e+02  1.32604610e+00\n",
            "   1.37998550e+02  1.76254975e+02  1.60595080e+00  1.49861025e+02\n",
            "   1.81414675e+02  2.72714120e+00  1.58100750e+02  1.85191825e+02\n",
            "   3.54854030e+00  1.53911025e+02  1.92869950e+02  3.66842680e+00\n",
            "   1.37788925e+02  1.90079650e+02  3.17300980e+00  1.30802075e+02\n",
            "   1.98029650e+02  2.82241230e+00  1.18779350e+02  1.95104650e+02\n",
            "   2.49634570e+00  1.10330975e+02  1.90919950e+02  2.26114210e+00\n",
            "   1.05108875e+02  1.86166825e+02  1.29335390e+00  1.18779350e+02\n",
            "   1.85377075e+02  1.00917090e+00  1.28691200e+02  1.85377075e+02\n",
            "   1.30536260e+00  1.37788925e+02  1.85377075e+02  1.90790930e+00\n",
            "   1.57335375e+02  1.85401450e+02  3.84124920e+00  1.37788925e+02\n",
            "   1.86191200e+02  3.55275340e+00  1.27877075e+02  1.86191200e+02\n",
            "   2.94505680e+00  1.18779350e+02  1.86191200e+02  2.67076900e+00]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "fl = np.loadtxt(\"landmarks_small_mouth.txt\").reshape((-1, 68,3))\n",
        "\n",
        "fl[:, :, 0:2] = -fl[:, :, 0:2]\n",
        "fl[:, :, 0:2] = fl[:, :, 0:2] / scale - shift\n",
        "\n",
        "fl0 = fl[0, :, :]\n",
        "\n",
        "fl0 = fl0.reshape(-1, 204)\n",
        "\n",
        "fls = []\n",
        "for fl1 in data_fls:\n",
        "    fls.append(fl0 + fl1)\n",
        "\n",
        "print(fls[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.0421875  -0.0265625  -0.01205357  0.00133929  0.01361607  0.02477679\n",
            "  0.03482143  0.04375     0.0515625   0.05825893  0.06383929  0.06830357\n",
            "  0.07165179  0.07388393  0.075       0.075       0.07388393  0.07165179\n",
            "  0.06830357  0.06383929  0.05825893  0.0515625   0.04375     0.03482143\n",
            "  0.02477679  0.01361607  0.00133929 -0.01205357 -0.0265625  -0.0421875 ]\n",
            "[-0.08571429  0.34285714  0.48571429  0.34285714 -0.08571429]\n",
            "Run on device cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "OpenCV: FFMPEG: tag 0x67706a6d/'mjpg' is not supported with codec id 7 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
            "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time - only video: 273.6616806983948\n",
            "Time - ffmpeg add audio: 274.04273676872253\n",
            "finish image2image gen\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "examples/.wav: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "fl = np.array(fls)\n",
        "\n",
        "fl = fl.reshape((-1, 68, 3))\n",
        "\n",
        "fl = util.add_naive_eye(fl)\n",
        "\n",
        "fl = fl.reshape((-1, 204))\n",
        "fl[:, :48 * 3] = savgol_filter(fl[:, :48 * 3], 30, 3, axis=0)\n",
        "fl[:, 48*3:] = savgol_filter(fl[:, 48*3:], 5, 3, axis=0)\n",
        "fl = fl.reshape((-1, 68, 3))\n",
        "\n",
        "''' STEP 6: Imag2image translation '''\n",
        "model = Image_translation_block(opt_parser, single_test=True)\n",
        "with torch.no_grad():\n",
        "    model.single_test(jpg=img, fls=fl, filename=\"landmarks_small_mouth.txt\", prefix=opt_parser.jpg.split('.')[0])\n",
        "    print('finish image2image gen')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m deltas_fl_to_0 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fl:\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# rever this:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# fl[:, :, 0:2] = -fl[:, :, 0:2]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# fl[:, :, 0:2] = fl[:, :, 0:2] / scale - shift\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     f[:, :, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m     f[:, :, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m f[:, :, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m scale \u001b[38;5;241m+\u001b[39m shift\n\u001b[1;32m     11\u001b[0m     deltas_fl_to_0\u001b[38;5;241m.\u001b[39mappend(f \u001b[38;5;241m-\u001b[39m fl0)\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
          ]
        }
      ],
      "source": [
        "deltas_fl_to_0 = []\n",
        "for f in fl:\n",
        "\n",
        "    # rever this:\n",
        "    # fl[:, :, 0:2] = -fl[:, :, 0:2]\n",
        "    # fl[:, :, 0:2] = fl[:, :, 0:2] / scale - shift\n",
        "    \n",
        "    f[:, :, 0:2] = -f[:, :, 0:2]\n",
        "    f[:, :, 0:2] = f[:, :, 0:2] * scale + shift\n",
        "\n",
        "    deltas_fl_to_0.append(f - fl0)\n",
        "\n",
        "print(deltas_fl_to_0[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "landmarks_list = [\n",
        "    \"fake_small_mouth/0.txt\",\n",
        "    \"fake_small_mouth/1.txt\",\n",
        "    \"fake_small_mouth/2.txt\",\n",
        "    \"fake_small_mouth/3.txt\",\n",
        "    \"fake_small_mouth/4.txt\",\n",
        "    \"fake_small_mouth/5.txt\",\n",
        "    \"fake_small_mouth/6.txt\",\n",
        "    \"fake_small_mouth/7.txt\",\n",
        "    \"fake_small_mouth/8.txt\",\n",
        "    \"fake_small_mouth/9.txt\",\n",
        "    \"fake_small_mouth/10.txt\",\n",
        "    \"fake_small_mouth/11.txt\",\n",
        "    \"fake_small_mouth/12.txt\",\n",
        "    \"fake_small_mouth/13.txt\",\n",
        "    \"fake_small_mouth/14.txt\",\n",
        "    \"fake_small_mouth/15.txt\",\n",
        "    \"fake_small_mouth/16.txt\",\n",
        "    \"fake_small_mouth/17.txt\",\n",
        "    \"fake_small_mouth/18.txt\",\n",
        "    \"fake_small_mouth/19.txt\",\n",
        "    \"fake_small_mouth/20.txt\",\n",
        "    \"fake_small_mouth/21.txt\",\n",
        "]\n",
        "\n",
        "data = [{\"audio\": 50, \"ID\": 0},\n",
        "\n",
        "{\"audio\": 100, \"ID\": 12},\n",
        "\n",
        "{\"audio\": 212.5, \"ID\": 4},\n",
        "\n",
        "{\"audio\": 262.5, \"ID\": 14},\n",
        "\n",
        "{\"audio\": 350, \"ID\": 8},\n",
        "\n",
        "{\"audio\": 512.5, \"ID\": 4},\n",
        "\n",
        "{\"audio\": 675, \"ID\": 0},\n",
        "\n",
        "{\"audio\": 1437.5, \"ID\": 0},\n",
        "\n",
        "{\"audio\": 1487.5, \"ID\": 11},\n",
        "\n",
        "{\"audio\": 1662.5, \"ID\": 1},\n",
        "\n",
        "{\"audio\": 1712.5, \"ID\": 21},\n",
        "\n",
        "{\"audio\": 1775, \"ID\": 6},\n",
        "\n",
        "{\"audio\": 1825, \"ID\": 3},\n",
        "\n",
        "{\"audio\": 1862.5, \"ID\": 13},\n",
        "\n",
        "{\"audio\": 1900, \"ID\": 18},\n",
        "\n",
        "{\"audio\": 1962.5, \"ID\": 5},\n",
        "\n",
        "{\"audio\": 2012.5, \"ID\": 13},\n",
        "\n",
        "{\"audio\": 2075, \"ID\": 19},\n",
        "\n",
        "{\"audio\": 2112.5, \"ID\": 16},\n",
        "\n",
        "{\"audio\": 2150, \"ID\": 4},\n",
        "\n",
        "{\"audio\": 2200, \"ID\": 1},\n",
        "\n",
        "{\"audio\": 2250, \"ID\": 14},\n",
        "\n",
        "{\"audio\": 2300, \"ID\": 18},\n",
        "\n",
        "{\"audio\": 2400, \"ID\": 13},\n",
        "\n",
        "{\"audio\": 2462.5, \"ID\": 4},\n",
        "\n",
        "{\"audio\": 2612.5, \"ID\": 19},\n",
        "\n",
        "{\"audio\": 2687.5, \"ID\": 19},\n",
        "\n",
        "{\"audio\": 2837.5, \"ID\": 0},\n",
        "\n",
        "{\"audio\": 3650, \"ID\": 0},\n",
        "\n",
        "{\"audio\": 3700, \"ID\": 11},\n",
        "\n",
        "{\"audio\": 3875, \"ID\": 1},\n",
        "\n",
        "{\"audio\": 3925, \"ID\": 21},\n",
        "\n",
        "{\"audio\": 3987.5, \"ID\": 12},\n",
        "\n",
        "{\"audio\": 4062.5, \"ID\": 6},\n",
        "\n",
        "{\"audio\": 4137.5, \"ID\": 13},\n",
        "\n",
        "{\"audio\": 4187.5, \"ID\": 19},\n",
        "\n",
        "{\"audio\": 4225, \"ID\": 4},\n",
        "\n",
        "{\"audio\": 4262.5, \"ID\": 12},\n",
        "\n",
        "{\"audio\": 4350, \"ID\": 4},\n",
        "\n",
        "{\"audio\": 4387.5, \"ID\": 14},\n",
        "\n",
        "{\"audio\": 4450, \"ID\": 21},\n",
        "\n",
        "{\"audio\": 4500, \"ID\": 6},\n",
        "\n",
        "{\"audio\": 4600, \"ID\": 7},\n",
        "\n",
        "{\"audio\": 4837, \"ID\": 0},\n",
        "\n",
        "{\"audio\": 5650, \"ID\": 0},\n",
        "\n",
        "{\"audio\": 5700, \"ID\": 11},\n",
        "\n",
        "{\"audio\": 5825, \"ID\": 19},\n",
        "\n",
        "{\"audio\": 5950, \"ID\": 8},\n",
        "\n",
        "{\"audio\": 5975, \"ID\": 4},\n",
        "\n",
        "{\"audio\": 6012.5, \"ID\": 17},\n",
        "\n",
        "{\"audio\": 6062.5, \"ID\": 1},\n",
        "\n",
        "{\"audio\": 6112.5, \"ID\": 19},\n",
        "\n",
        "{\"audio\": 6162.5, \"ID\": 11},\n",
        "\n",
        "{\"audio\": 6225, \"ID\": 19},\n",
        "\n",
        "{\"audio\": 6325, \"ID\": 6},\n",
        "\n",
        "{\"audio\": 6400, \"ID\": 19},\n",
        "\n",
        "{\"audio\": 6462.5, \"ID\": 15},\n",
        "\n",
        "{\"audio\": 6537.5, \"ID\": 1},\n",
        "\n",
        "{\"audio\": 6587.5, \"ID\": 21},\n",
        "\n",
        "{\"audio\": 6637.5, \"ID\": 6},\n",
        "\n",
        "{\"audio\": 6687.5, \"ID\": 21},\n",
        "\n",
        "{\"audio\": 6737.5, \"ID\": 21},\n",
        "\n",
        "{\"audio\": 6800, \"ID\": 13},\n",
        "\n",
        "{\"audio\": 6862.5, \"ID\": 7},\n",
        "\n",
        "{\"audio\": 6912.5, \"ID\": 18},\n",
        "\n",
        "{\"audio\": 6950, \"ID\": 21},\n",
        "\n",
        "{\"audio\": 7000, \"ID\": 1},\n",
        "\n",
        "{\"audio\": 7087.5, \"ID\": 19},\n",
        "\n",
        "{\"audio\": 7150, \"ID\": 19},\n",
        "\n",
        "{\"audio\": 7200, \"ID\": 15},\n",
        "\n",
        "{\"audio\": 7375, \"ID\": 0}\n",
        "]\n",
        "\n",
        "from moviepy.editor import VideoFileClip, ImageSequenceClip\n",
        "from moviepy.editor import *\n",
        "\n",
        "frames = []\n",
        "\n",
        "# List to hold the video clips\n",
        "clips = []\n",
        "\n",
        "#create a list of all the frames (30 fps) in 7500 ms\n",
        "num_frames = int(30 * 7.5)\n",
        "frames = []\n",
        "\n",
        "list_landmarks = []\n",
        "for i in range(num_frames):\n",
        "\n",
        "    # get the viseme id for the specific frame based on the time of the data\n",
        "    viseme_id = 0\n",
        "    for data_point in data:\n",
        "        if data_point[\"audio\"] > i * 1000 / 30:\n",
        "            viseme_id = data_point[\"ID\"]\n",
        "            break\n",
        "    \n",
        "    # get the landmark file for the specific viseme\n",
        "    landmark_file = landmarks_list[viseme_id]\n",
        "    landmark_data = np.loadtxt(landmark_file).tolist()\n",
        "    \n",
        "    list_landmarks.append(landmark_data)\n",
        "\n",
        "# write to a txt file (one line per frame)\n",
        "with open(\"landmarks_small_mouth.txt\", \"w\") as f:\n",
        "    for landmark in list_landmarks:\n",
        "        for point in landmark:\n",
        "            for p in point:\n",
        "                f.write(str(p) + \" \")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "# # Concatenate all the clips\n",
        "# final_video = concatenate_videoclips(clips, method=\"compose\")\n",
        "\n",
        "# # Export the video\n",
        "# final_video.write_videofile(\"final_output_video_2.mp4\", fps=24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for id in range(0, 22):\n",
        "#     img =cv2.imread('fake/'+ str(id) + '.png')\n",
        "#     predictor = face_alignment.FaceAlignment(face_alignment.LandmarksType.THREE_D, device='cpu', flip_input=True)\n",
        "#     shapes = predictor.get_landmarks(img)\n",
        "#     if (not shapes or len(shapes) != 1):\n",
        "#         print('Cannot detect face landmarks. Exit.')\n",
        "#         exit(-1)\n",
        "#     shape_3d = shapes[0]\n",
        "\n",
        "#     util.close_input_face_mouth(shape_3d)\n",
        "\n",
        "#     # make the mouth a little bit more open\n",
        "#     # shape_3d[48:60, 1] -= 0.2\n",
        "#     # shape_3d[48:60, 2] += 0.2\n",
        "\n",
        "#     shape_3d[48 * 3::3] *= 6  # mouth x\n",
        "#     shape_3d[48 * 3 + 1::3] *= 6  # mouth y\n",
        "\n",
        "#     shape_3d, scale, shift = util.norm_input_face(shape_3d)\n",
        "\n",
        "#     shape_3d = shape_3d.reshape(-1, 204)[0]\n",
        "\n",
        "#     # write to a txt file in one line with 5 digits after the decimal point\n",
        "#     np.savetxt('fake/'+ str(id) + '.txt', shape_3d, fmt='%.5f', newline=' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_9LmmACg9Mq"
      },
      "source": [
        "## (Optional) Simple manual adjustment to landmarks in case FAN is not accurate, e.g.\n",
        "- slimmer lips\n",
        "- wider eyes\n",
        "- wider mouth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2PLXNlhgztJ"
      },
      "outputs": [],
      "source": [
        "shape_3d[48:, 0] = (shape_3d[48:, 0] - np.mean(shape_3d[48:, 0])) * 1.05 + np.mean(shape_3d[48:, 0]) # wider lips\n",
        "shape_3d[49:54, 1] += 0.           # thinner upper lip\n",
        "shape_3d[55:60, 1] -= 1.           # thinner lower lip\n",
        "shape_3d[[37,38,43,44], 1] -=2.    # larger eyes\n",
        "shape_3d[[40,41,46,47], 1] +=2.    # larger eyes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nlaLLoShR1k"
      },
      "source": [
        "Normalize face as input to audio branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0GkD0fThN-2"
      },
      "outputs": [],
      "source": [
        "shape_3d, scale, shift = util.norm_input_face(shape_3d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAcGrT3PhY3T"
      },
      "source": [
        "## Step 3: Generate input data for inference based on uploaded audio `MakeItTalk/examples/*.wav`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mqh5A_7chQ8g",
        "outputId": "e7a357f9-dbc7-4597-a7e9-184e69b705ba"
      },
      "outputs": [],
      "source": [
        "au_data = []\n",
        "au_emb = []\n",
        "ains = glob.glob1('examples', '*.wav')\n",
        "ains = [item for item in ains if item is not 'tmp.wav']\n",
        "ains.sort()\n",
        "for ain in ains:\n",
        "    os.system('/opt/homebrew/bin/ffmpeg -y -loglevel error -i examples/{} -ar 16000 examples/tmp.wav'.format(ain))\n",
        "    shutil.copyfile('examples/tmp.wav', 'examples/{}'.format(ain))\n",
        "\n",
        "    # au embedding\n",
        "    from thirdparty.resemblyer_util.speaker_emb import get_spk_emb\n",
        "    me, ae = get_spk_emb('examples/{}'.format(ain))\n",
        "    au_emb.append(me.reshape(-1))\n",
        "\n",
        "    print('Processing audio file', ain)\n",
        "    c = AutoVC_mel_Convertor('examples')\n",
        "\n",
        "    au_data_i = c.convert_single_wav_to_autovc_input(audio_filename=os.path.join('examples', ain),\n",
        "           autovc_model_path=opt_parser.load_AUTOVC_name)\n",
        "    au_data += au_data_i\n",
        "if(os.path.isfile('examples/tmp.wav')):\n",
        "    os.remove('examples/tmp.wav')\n",
        "\n",
        "# landmark fake placeholder\n",
        "fl_data = []\n",
        "rot_tran, rot_quat, anchor_t_shape = [], [], []\n",
        "for au, info in au_data:\n",
        "    au_length = au.shape[0]\n",
        "    fl = np.zeros(shape=(au_length, 68 * 3))\n",
        "    fl_data.append((fl, info))\n",
        "    rot_tran.append(np.zeros(shape=(au_length, 3, 4)))\n",
        "    rot_quat.append(np.zeros(shape=(au_length, 4)))\n",
        "    anchor_t_shape.append(np.zeros(shape=(au_length, 68 * 3)))\n",
        "\n",
        "if(os.path.exists(os.path.join('examples', 'dump', 'random_val_fl.pickle'))):\n",
        "    os.remove(os.path.join('examples', 'dump', 'random_val_fl.pickle'))\n",
        "if(os.path.exists(os.path.join('examples', 'dump', 'random_val_fl_interp.pickle'))):\n",
        "    os.remove(os.path.join('examples', 'dump', 'random_val_fl_interp.pickle'))\n",
        "if(os.path.exists(os.path.join('examples', 'dump', 'random_val_au.pickle'))):\n",
        "    os.remove(os.path.join('examples', 'dump', 'random_val_au.pickle'))\n",
        "if (os.path.exists(os.path.join('examples', 'dump', 'random_val_gaze.pickle'))):\n",
        "    os.remove(os.path.join('examples', 'dump', 'random_val_gaze.pickle'))\n",
        "\n",
        "with open(os.path.join('examples', 'dump', 'random_val_fl.pickle'), 'wb') as fp:\n",
        "    pickle.dump(fl_data, fp)\n",
        "with open(os.path.join('examples', 'dump', 'random_val_au.pickle'), 'wb') as fp:\n",
        "    pickle.dump(au_data, fp)\n",
        "with open(os.path.join('examples', 'dump', 'random_val_gaze.pickle'), 'wb') as fp:\n",
        "    gaze = {'rot_trans':rot_tran, 'rot_quat':rot_quat, 'anchor_t_shape':anchor_t_shape}\n",
        "    pickle.dump(gaze, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNzY0KtMhkkV"
      },
      "source": [
        "## Step 4: Audio-to-Landmarks prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP94GnGchXy8",
        "outputId": "10c1dc3d-4f60-4f13-f9ba-8e03b8cca18f"
      },
      "outputs": [],
      "source": [
        "!pwd\n",
        "model = Audio2landmark_model(opt_parser, jpg_shape=shape_3d)\n",
        "if(len(opt_parser.reuse_train_emb_list) == 0):\n",
        "    model.test(au_emb=au_emb)\n",
        "else:\n",
        "    model.test(au_emb=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vis_landmark_on_img(img, shape, linewidth=2):\n",
        "    '''\n",
        "    Visualize landmark on images.\n",
        "    '''\n",
        "\n",
        "    def draw_curve(idx_list, color=(0, 255, 0), loop=False, lineWidth=linewidth):\n",
        "        for i in idx_list:\n",
        "            cv2.line(img, (shape[i, 0], shape[i, 1]), (shape[i + 1, 0], shape[i + 1, 1]), color, lineWidth)\n",
        "        if (loop):\n",
        "            cv2.line(img, (shape[idx_list[0], 0], shape[idx_list[0], 1]),\n",
        "                     (shape[idx_list[-1] + 1, 0], shape[idx_list[-1] + 1, 1]), color, lineWidth)\n",
        "\n",
        "    draw_curve(list(range(0, 16)), color=(255, 144, 25))  # jaw\n",
        "    draw_curve(list(range(17, 21)), color=(50, 205, 50))  # eye brow\n",
        "    draw_curve(list(range(22, 26)), color=(50, 205, 50))\n",
        "    draw_curve(list(range(27, 35)), color=(208, 224, 63))  # nose\n",
        "    draw_curve(list(range(36, 41)), loop=True, color=(71, 99, 255))  # eyes\n",
        "    draw_curve(list(range(42, 47)), loop=True, color=(71, 99, 255))\n",
        "    draw_curve(list(range(48, 59)), loop=True, color=(238, 130, 238))  # mouth\n",
        "    draw_curve(list(range(60, 67)), loop=True, color=(238, 130, 238))\n",
        "\n",
        "    return img\n",
        "\n",
        "img = cv2.imread('real/2.png')\n",
        "\n",
        "fl = np.loadtxt(\"landmarks.txt\").reshape((-1, 68,3))\n",
        "fl[:, :, 0:2] = -fl[:, :, 0:2]\n",
        "fl[:, :, 0:2] = fl[:, :, 0:2] / scale - shift\n",
        "\n",
        "if (ADD_NAIVE_EYE):\n",
        "    fl = util.add_naive_eye(fl)\n",
        "\n",
        "# additional smooth\n",
        "fl = fl.reshape((-1, 204))\n",
        "fl[:, :48 * 3] = savgol_filter(fl[:, :48 * 3], 15, 3, axis=0)\n",
        "fl[:, 48*3:] = savgol_filter(fl[:, 48*3:], 5, 3, axis=0)\n",
        "fl = fl.reshape((-1, 68, 3))\n",
        "\n",
        "fl = fl.astype(int)\n",
        "img_fl = np.ones(shape=(256, 256, 3)) * 255\n",
        "img_fl = img_fl.astype(int)\n",
        "img_fl = vis_landmark_on_img(img_fl, fl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "fl = np.loadtxt(\"landmarks_small_mouth.txt\").reshape((-1, 68,3))\n",
        "\n",
        "fl[:, :, 0:2] = -fl[:, :, 0:2]\n",
        "fl[:, :, 0:2] = fl[:, :, 0:2] / scale - shift\n",
        "\n",
        "# if (ADD_NAIVE_EYE):\n",
        "#     fl = util.add_naive_eye(fl)\n",
        "\n",
        "window_size = 10\n",
        "\n",
        "# Iterate over each frame\n",
        "for i in range(len(fl_filtered)):\n",
        "    # Determine the start and end indices of the window\n",
        "    if i < window_size:\n",
        "        continue\n",
        "\n",
        "    # Extract the subset for filtering\n",
        "    subset = fl_filtered[i-10:i]\n",
        "\n",
        "    # Apply Savitzky-Golay filter to the subset\n",
        "    subset[:, :48 * 3] = savgol_filter(subset[:, :48 * 3], 10, 3, axis=0)\n",
        "    subset[:, 48*3:] = savgol_filter(subset[:, 48*3:], 10, 3, axis=0)\n",
        "\n",
        "    fl_filtered[i] = subset[i]\n",
        "\n",
        "# Reshape back to the original shape\n",
        "fl = fl_filtered.reshape((-1, 68, 3))\n",
        "\n",
        "''' STEP 6: Imag2image translation '''\n",
        "model = Image_translation_block(opt_parser, single_test=True)\n",
        "with torch.no_grad():\n",
        "    model.single_test(jpg=img, fls=fl, filename=\"landmarks.txt\", prefix=opt_parser.jpg.split('.')[0])\n",
        "    print('finish image2image gen')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFaYlUNNjnxn"
      },
      "source": [
        "## Step 5: Natural face animation via Image-to-image translation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xYBO_czjFSD",
        "outputId": "1810cbba-4876-4ecd-d6ef-c55cd95a6e1b"
      },
      "outputs": [],
      "source": [
        "# fls = glob.glob1('examples', 'pred_fls_*.txt')\n",
        "# fls.sort()\n",
        "\n",
        "# for i in range(0,len(fls)):\n",
        "#     fl = np.loadtxt(os.path.join('examples', fls[i])).reshape((-1, 68,3))\n",
        "#     print(fl.shape)\n",
        "#     fl[:, :, 0:2] = -fl[:, :, 0:2]\n",
        "#     fl[:, :, 0:2] = fl[:, :, 0:2] / scale - shift\n",
        "\n",
        "#     if (ADD_NAIVE_EYE):\n",
        "#         fl = util.add_naive_eye(fl)\n",
        "\n",
        "#     # additional smooth\n",
        "#     fl = fl.reshape((-1, 204))\n",
        "#     fl[:, :48 * 3] = savgol_filter(fl[:, :48 * 3], 15, 3, axis=0)\n",
        "#     fl[:, 48*3:] = savgol_filter(fl[:, 48*3:], 5, 3, axis=0)\n",
        "#     fl = fl.reshape((-1, 68, 3))\n",
        "\n",
        "#     ''' STEP 6: Imag2image translation '''\n",
        "#     model = Image_translation_block(opt_parser, single_test=True)\n",
        "#     with torch.no_grad():\n",
        "#         model.single_test(jpg=img, fls=fl, filename=fls[i], prefix=opt_parser.jpg.split('.')[0])\n",
        "#         print('finish image2image gen')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8mMguI_j1TQ"
      },
      "source": [
        "## Visualize your animation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "Xmnr2CsChmnB",
        "outputId": "c7decb3d-102e-484c-9b25-56961d17df3b"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "for ain in ains:\n",
        "  OUTPUT_MP4_NAME = '{}_pred_fls_{}_audio_embed.mp4'.format(\n",
        "    opt_parser.jpg.split('.')[0],\n",
        "    ain.split('.')[0]\n",
        "    )\n",
        "  mp4 = open('examples/{}'.format(OUTPUT_MP4_NAME),'rb').read()\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "  print('Display animation: examples/{}'.format(OUTPUT_MP4_NAME))\n",
        "  display(HTML(\"\"\"\n",
        "  <video width=600 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\" % data_url))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxWMuEEbpywq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOYW4P15IPg+x69aFu7awQb",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "quick_demo.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
